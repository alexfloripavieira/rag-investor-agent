#!/usr/bin/env python3
"""
Script para testar a geraÃ§Ã£o de insights dos relatÃ³rios
"""
import os
from dotenv import load_dotenv
from vector_store import VectorStoreManager
import llm_services

# Carregar variÃ¡veis de ambiente
load_dotenv()

def test_insights_generation():
    """Testa a geraÃ§Ã£o de insights dos relatÃ³rios."""
    print("ğŸ’¡ TESTE DE GERAÃ‡ÃƒO DE INSIGHTS")
    print("=" * 60)
    
    # Verificar se OpenAI API key estÃ¡ configurada
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ OPENAI_API_KEY nÃ£o configurada!")
        return
    print("âœ… OpenAI API Key configurada")
    
    # Inicializar vector store
    print("\n1. Inicializando Vector Store...")
    vector_manager = VectorStoreManager()
    doc_count = vector_manager.count_documents()
    
    if doc_count == 0:
        print("âš ï¸ Nenhum documento no vector store. Execute test_add_document.py primeiro.")
        return
    
    print(f"ğŸ“Š Vector Store com {doc_count} chunks prontos")
    
    # Obter retriever
    retriever = vector_manager.get_retriever(k=4)
    
    print("\n2. Testando Resumo Executivo...")
    try:
        summary = llm_services.generate_market_summary(retriever)
        print("âœ… Resumo executivo gerado com sucesso")
        print(f"ğŸ“„ Tamanho: {len(summary)} caracteres")
        print(f"ğŸ” Preview: {summary[:200]}...")
        print()
    except Exception as e:
        print(f"âŒ Erro no resumo executivo: {e}")
    
    print("\n3. Testando ExtraÃ§Ã£o de MÃ©tricas...")
    try:
        metrics = llm_services.extract_key_metrics(retriever)
        if "error" in metrics:
            print(f"âŒ Erro nas mÃ©tricas: {metrics['error']}")
        else:
            print("âœ… MÃ©tricas extraÃ­das com sucesso")
            metrics_text = metrics.get("metrics", "")
            print(f"ğŸ“Š Tamanho: {len(metrics_text)} caracteres")
            print(f"ğŸ”¢ Preview: {metrics_text[:200]}...")
        print()
    except Exception as e:
        print(f"âŒ Erro na extraÃ§Ã£o de mÃ©tricas: {e}")
    
    print("\n4. Testando Insights Detalhados...")
    try:
        insights = llm_services.generate_insights_from_documents(retriever)
        print(f"âœ… {len(insights)} insights detalhados gerados")
        
        for i, (query, insight) in enumerate(insights.items(), 1):
            print(f"\nğŸ“‹ Insight {i}: {query}")
            if insight.startswith("Erro"):
                print(f"   âŒ {insight}")
            else:
                print(f"   âœ… Gerado ({len(insight)} caracteres)")
                print(f"   ğŸ” Preview: {insight[:150]}...")
                
    except Exception as e:
        print(f"âŒ Erro nos insights detalhados: {e}")
    
    print("\n5. Teste de Performance...")
    import time
    
    start_time = time.time()
    test_docs = retriever.invoke("FII investimento rentabilidade")
    retrieval_time = time.time() - start_time
    
    print(f"ğŸ” RecuperaÃ§Ã£o: {len(test_docs)} docs em {retrieval_time:.2f}s")
    
    # Teste simples de geraÃ§Ã£o
    start_time = time.time()
    try:
        quick_summary = llm_services.generate_market_summary(retriever)
        generation_time = time.time() - start_time
        print(f"ğŸ¤– GeraÃ§Ã£o: {len(quick_summary)} chars em {generation_time:.2f}s")
    except Exception as e:
        print(f"âŒ Erro no teste de performance: {e}")
    
    print("\n" + "=" * 60)
    print("ğŸ¯ TESTE DE INSIGHTS CONCLUÃDO")
    print("\nğŸ’¡ Dicas para melhorar insights:")
    print("   â€¢ Adicione mais documentos para anÃ¡lises mais ricas")
    print("   â€¢ Documentos de diferentes perÃ­odos mostram tendÃªncias")
    print("   â€¢ Dados estruturados geram mÃ©tricas mais precisas")

if __name__ == "__main__":
    test_insights_generation()